{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a9bade-e2fc-4f83-8f09-6805934625c6",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation\n",
    "\n",
    "In this page we will discuss what K-Fold Cross-Validation is. We will be deriving a lot of information from [this website.](https://machinelearningmastery.com/k-fold-cross-validation/) Also worth noting that [Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html) also has some good information on what Cross-Validation is. Also, the Scikit-Learn website has some incredibly useful diagrams for illustrating how K-Fold Cross-Validation is.\n",
    "\n",
    "As a short summary, K-Fold Cross-Validation is a statistical model used to estimate how accurate a Machine Learning Model is. That is admittedly a little vague, so allow me a more detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999e95c-052b-4a05-af39-124646dba2b5",
   "metadata": {},
   "source": [
    "### Detailed Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998dadd-df44-46b0-90a6-b2d3c69130c3",
   "metadata": {},
   "source": [
    "Without going into the \"theory\" of it, K-Fold Cross-Validation is when you take the training data for a Machine Learning Model, and then break it up into k number of folds. Then, you train the model on k - 1 number of folds of the training data, and you use the remaining fold as the validation set. Then you iterate on this - next you pick a different fold as the validation set, and then you train the model on all of the remaning folds of training data. Repeat until every fold has been used as a validation set.\n",
    "\n",
    "So let's use an example - suppose we were trying to train a model and we had our training data. Let's split it up into five folds - Folds A, B, C, D and E. \n",
    "\n",
    "For our first pass, we use Fold A as our validation data (I.E., the data to see if the model was able to make predictions properly), and we use Folds B to E as the data we train the model on. \n",
    "\n",
    "For the second pass, we use Fold B as our validation data, and then train the model using Folds A, C, D and E. We repeat this process until we've used every fold of data as a validation set.\n",
    "\n",
    "The benefits of doing this is that we don't have to waste much data as validation data, (in the sense that the data is only validation data; we \"double dip\" the data and use it as training and validation data) but this can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb741b6e-571c-41c7-b699-df4f1c343c98",
   "metadata": {},
   "source": [
    "### Further Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc604ed-18f6-4812-a285-d434fe6bd855",
   "metadata": {},
   "source": [
    "As described by [Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html), there are multiple ways to validate machine learning models, the K-Fold Cross-Validation is just one way to do it.\n",
    "\n",
    "There's no written rule on how many folds you should have, but it seems common practice to take either 5 or 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f77cd-8267-4d79-802c-8fdcbcb84b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
