{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91eb046d-a073-4444-8200-be0fcf63e278",
   "metadata": {},
   "source": [
    "# Cross-Entropy (for Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81696f-da25-42a6-b61f-f323a4802931",
   "metadata": {},
   "source": [
    "Cross-Entropy in Machine Learning is a type of Loss Function - remember that a Loss Function Loss Function is a method of evaluating how well a machine learning algorithm models a dataset.\n",
    "\n",
    "There is a great explanation on [towards data science](https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616), but essentially what cross-entropy is really doing is tracking events and probabilities, and then asking how likely an event is to occur based on probabilities. I've skipped a lot of the details, but if an event is very likely to occur based on some probabilities then it has a _small_ cross-entropy, and if it's not very likely to occur then it has a _high_ cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c331511-ff9d-414c-89ac-58dbb11e35d1",
   "metadata": {},
   "source": [
    "### Brief run down of the Math behind Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11099315-8280-465e-8f71-b1f69ac69731",
   "metadata": {},
   "source": [
    "Normally, to determine the likelyhood of something occurring, you'd look at the probability. If you were looking to determine the odds of something occurring after (for example) two specific probabilities, you would multiply the two probabilities together. So if I were to consider the event of getting heads on a coin and then rolling a six on a dice, the odds of the two happening together (assuming we don't do any advanced statistics) would be `1/2 * 1/6`.\n",
    "\n",
    "Now in Machine Learning, we might have hundreds of probabilities, so multiplying them all together may be ill advised. For this reason, we turn to logarithms. If you're unfamiliar with logarithms, [you can find more about them here.](https://www.rapidtables.com/math/algebra/Logarithm.html)\n",
    "\n",
    "So if we consider our above example, instead of multiplying probabilities, if we take the natural logs of them we would have: `ln(1/2) + ln(1/6)`, which would work out to be `-2.484`. The negative values are a little harder to understand though, so we end up multiplying everything by `-1`. So, we have `-ln(1/2) - ln(1/6) = 2.484`.\n",
    "\n",
    "In the case of machine learning, the _lower_ the number is, the better the model, and the _higher_ the number is, the worse the model is.\n",
    "\n",
    "Again, for a better look at the logarithmic formulas behind Cross-Entropy, I strongly recommend [checking this article out.](https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3cf6a-a317-48e7-a4b0-4076ebf21095",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d488e-5852-4ce7-9179-afe06851c348",
   "metadata": {},
   "source": [
    "What is binary cross entropy? \n",
    "\n",
    "[From what it looks like,](https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/#:~:text=What%20is%20Binary%20Cross%20Entropy,far%20from%20the%20actual%20value.) Binary Cross-Entropy looks like it's just Cross-Entropy but just with 0's or 1's. Which, that tracks - binary would be 1 or 0.\n",
    "\n",
    "[This article] does a good job of going in-depth on this, but basically this is like regular Cross-Entropy, but just with 0's and 1's. 1's being 100% probability and 0's being zero percent probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec98669-194e-4f48-84c5-b15be4f86117",
   "metadata": {},
   "source": [
    "### Additional Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c73df9-81a1-463f-8c8d-61b5f6d5fb8c",
   "metadata": {},
   "source": [
    "- [Machine Learning Mastery's notes on Cross-Entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) (This also has examples on how to implement this in Python)\n",
    "- [Towards Data Science's notes on Cross-Entropy](https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616)\n",
    "- [Analytics in Diamag's notes on Cross-Entropy](https://analyticsindiamag.com/a-beginners-guide-to-cross-entropy-in-machine-learning/)\n",
    "- [Neptune AI's notes on Cross-Entropy](https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning) (Very clear and easy to understand examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8b1de-80c6-4fda-89c2-5851dbb47e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
