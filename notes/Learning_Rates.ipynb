{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642dcc1d-ed9a-4673-8bb5-38a6bab65302",
   "metadata": {},
   "source": [
    "# Learning Rates in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dc101-5f2f-4497-81cd-d343b90c55ec",
   "metadata": {},
   "source": [
    "[According to Wikipedia,](https://en.wikipedia.org/wiki/Learning_rate) the learning rate is a tuning parameter in an optimization algorithm that determines the step size of each iteration while moving towards a minimum of a loss function.\n",
    "\n",
    "[The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)\n",
    "\n",
    "So put very _loosely_, the learning rate determines how fast/quickly a model processes information. If you process too much data too quickly, you might arrive at a misleading or false conclusion, but if you process data too slowly then the modle could stall out and not make meaningful progress.\n",
    "\n",
    "Put more scientifically,\n",
    "\n",
    "[The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f26e4-5ef2-4986-b89d-edfe719416ac",
   "metadata": {},
   "source": [
    "### Stocastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292643ad-3561-4e7b-8192-2ef3c194da05",
   "metadata": {},
   "source": [
    "Deep learning neural networks are trained using the stochastic gradient descent algorithm. Note that \"Stochastic Gradient Descent\" is often abbreviated to \"SGD\". Note [this article](https://towardsdatascience.com/regression-explained-in-simple-terms-dccbcad96f61) on Linear Regression is worth thumbing through to understand SGD - very simply Linear Regression can be used to find relationships between variables.\n",
    "\n",
    "Also note that “Stochastic”, in plain terms means “random”.\n",
    "\n",
    "Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm, referred to as simply backpropagation.\n",
    "\n",
    "[Towards DataScience has a really good explanation for the Stocastic Gradient Descent.](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)\n",
    "\n",
    "In short though, Stocastic Gradient Descent is a tool to reduce the error of whatever Machine Learning Model we use, and isn't dissimilar to Linear Regression.\n",
    "\n",
    "There is more explanation on it [here](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/) and [here.](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n",
    "\n",
    "As a final note, in relation to learning rate, [The “learning rate” mentioned above is a flexible parameter which heavily influences the convergence of the algorithm. Larger learning rates make the algorithm take huge steps down the slope and it might jump across the minimum point thereby missing it. So, it is always good to stick to low learning rate such as 0.01. It can also be mathematically shown that gradient descent algorithm takes larger steps down the slope if the starting point is high above and takes baby steps as it reaches closer to the destination to be careful not to miss it and also be quick enough.](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e1e10-a44f-4fff-bc77-56005f4a5b3a",
   "metadata": {},
   "source": [
    "### Learning Rate in Keras (and by extension, Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e60ec-65f2-4612-b0c7-5f887b1e66f0",
   "metadata": {},
   "source": [
    "While there's a lot more information on [machine learning mastery,](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/) it is worth mentioning that there is a Stochastic Gradient Descent class in Keras, and you can access it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab62ad3-574c-4cb6-9a00-7582a8a957a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "...\n",
    "opt = SGD()\n",
    "model.compile(..., optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afed18f-9736-4d5f-ab68-ff644738aa42",
   "metadata": {},
   "source": [
    "Note that you can add \"learning rate\" (lr), \"momentum\" and \"decay\" parameters to the SGD class. The \"decay\" parameter slows down the learning rate, so that we don't \"overshoot\" the case in which the gradient is 0. The formula is as follows: `lrate = initial_lrate * (1 / (1 + decay * iteration))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6980df-f048-40e9-9f66-9c1388b06db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "...\n",
    "opt = SGD(lr=0.01, momentum=0.9, decay=0.01)\n",
    "model.compile(..., optimizer=opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
